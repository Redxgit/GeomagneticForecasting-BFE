{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace75cd",
   "metadata": {},
   "source": [
    "__author__ = 'Armando Collado-Villaverde'\n",
    "__email__ = 'armando.collado@uah.es'\n",
    "\n",
    "# Download the SYM-H storms dataset\n",
    "\n",
    "Downloads the whole timeline for the SYM-H index which will be used in the `1-set-expansion.ipynb` notebook, the ACE MAG and SWEPAM datasets and the SYM-H for the selected storms. The data will be downloaded from NASA's CDAWeb using the cdasws package. The specific datasets are as follows:\n",
    "\n",
    "* The data for the SYM-H index will be downloaded from the `OMNI_HRO_5MIN` dataset\n",
    "* For the ACE MAG datasets:\n",
    "    * For the definitive data we use the `AC_H0_MFI`\n",
    "    * For the provisional data we use the `AC_K1_MFI`\n",
    "* For the ACE SWEPAM datasets:\n",
    "    * For the definitive data we use the `AC_H0_SWE`\n",
    "    * For the provisional data we use the `AC_K0_SWE`\n",
    "\n",
    "First the raw data will be downloaded in saved: [Download and save raw data](#section_1)\n",
    "Then the data from ACE will be grouped in 5 minute averages, they are calculated with right-closed intervals, then the data is merged with the index and saved for later use: [Preprocess and save data](#section_2)\n",
    "\n",
    "The used directories are specified in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16b8504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:45:55.122365Z",
     "start_time": "2024-01-17T09:45:54.545735Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isg/anaconda3/envs/tf2-14/lib/python3.10/site-packages/spacepy/time.py:2367: UserWarning: Leapseconds may be out of date. Use spacepy.toolbox.update(leapsecs=True)\n",
      "  warnings.warn('Leapseconds may be out of date.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import utils\n",
    "from cdasws import CdasWs\n",
    "import pandas as pd\n",
    "from cdasws.datarepresentation import DataRepresentation as dr\n",
    "import numpy as np\n",
    "import storm_dates\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43935e6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T10:43:23.564976Z",
     "start_time": "2024-01-17T10:43:23.560577Z"
    }
   },
   "outputs": [],
   "source": [
    "download_base_path = \"./data/\"\n",
    "preprocessed_data_path = os.path.join(download_base_path, 'preprocessed_data')\n",
    "cdas = CdasWs()\n",
    "\n",
    "train_data_path = os.path.join(download_base_path, 'sym_h_storms/train_storms')\n",
    "val_data_path = os.path.join(download_base_path, 'sym_h_storms/validation_storms')\n",
    "test_data_path = os.path.join(download_base_path, 'sym_h_storms/test_storms')\n",
    "test_key_data_path = os.path.join(download_base_path, 'sym_h_storms/test_key_storms')\n",
    "\n",
    "train_data_path_preprocessed = os.path.join(preprocessed_data_path, 'train_storms')\n",
    "val_data_path_preprocessed = os.path.join(preprocessed_data_path, 'validation_storms')\n",
    "test_data_path_preprocessed = os.path.join(preprocessed_data_path, 'test_storms')\n",
    "test_key_data_path_preprocessed = os.path.join(preprocessed_data_path, 'test_key_storms')\n",
    "\n",
    "train_data_path_preprocessed = os.path.join(preprocessed_data_path, 'train_storms')\n",
    "val_data_path_preprocessed = os.path.join(preprocessed_data_path, 'validation_storms')\n",
    "test_data_path_preprocessed = os.path.join(preprocessed_data_path, 'test_storms')\n",
    "test_key_data_path_preprocessed = os.path.join(preprocessed_data_path, 'test_key_storms')\n",
    "\n",
    "os.makedirs(train_data_path_preprocessed, exist_ok = True)\n",
    "os.makedirs(val_data_path_preprocessed, exist_ok = True)\n",
    "os.makedirs(test_data_path_preprocessed, exist_ok = True)\n",
    "os.makedirs(test_key_data_path_preprocessed, exist_ok = True)\n",
    "\n",
    "os.makedirs(download_base_path, exist_ok=True)\n",
    "os.makedirs(preprocessed_data_path, exist_ok=True)\n",
    "\n",
    "INDEX_COL = ['SYM_H']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e74c9",
   "metadata": {},
   "source": [
    "# Download and save raw data <a class=\"anchor\" id=\"section_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cec3e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T09:56:01.664327Z",
     "start_time": "2024-01-17T09:45:55.127979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving raw data to ./data/\n",
      "Downloading SYM-H data from 1998-01-01 00:00:00+00:00 until 2023-01-01 00:00:00+00:00\n",
      "Data will be downloaded from the 'OMNI_HRO_5MIN' dataset with a 5 minute resolution\n",
      "Saving data to ./data/sym_index.pkl\n",
      "\tDownloading SYM storms\n",
      "\t\tTrain storms ./data/sym_h_storms/train_storms\n",
      "\t\t\tSaving storm 1\n",
      "\t\t\tSaving storm 2\n",
      "\t\t\tSaving storm 3\n",
      "\t\t\tSaving storm 4\n",
      "\t\t\tSaving storm 5\n",
      "\t\t\tSaving storm 6\n",
      "\t\t\tSaving storm 7\n",
      "\t\t\tSaving storm 8\n",
      "\t\t\tSaving storm 9\n",
      "\t\t\tSaving storm 10\n",
      "\t\t\tSaving storm 11\n",
      "\t\t\tSaving storm 12\n",
      "\t\t\tSaving storm 13\n",
      "\t\t\tSaving storm 14\n",
      "\t\t\tSaving storm 15\n",
      "\t\t\tSaving storm 16\n",
      "\t\t\tSaving storm 17\n",
      "\t\t\tSaving storm 18\n",
      "\t\t\tSaving storm 19\n",
      "\t\t\tSaving storm 20\n",
      "\t\t\tSaving storm 21\n",
      "\t\t\tSaving storm 22\n",
      "\t\t\tSaving storm 23\n",
      "\t\t\tSaving storm 24\n",
      "\t\t\tSaving storm 25\n",
      "\t\t\tSaving storm 26\n",
      "\t\t\tSaving storm 27\n",
      "\t\t\tSaving storm 28\n",
      "\t\t\tSaving storm 29\n",
      "\t\t\tSaving storm 30\n",
      "\t\t\tSaving storm 31\n",
      "\t\t\tSaving storm 32\n",
      "\t\t\tSaving storm 33\n",
      "\t\t\tSaving storm 34\n",
      "\t\t\tSaving storm 35\n",
      "\t\t\tSaving storm 36\n",
      "\t\t\tSaving storm 37\n",
      "\t\t\tSaving storm 38\n",
      "\t\t\tSaving storm 39\n",
      "\t\t\tSaving storm 40\n",
      "\t\t\tSaving storm 41\n",
      "\t\t\tSaving storm 42\n",
      "\t\t\tSaving storm 43\n",
      "\t\t\tSaving storm 44\n",
      "\t\t\tSaving storm 45\n",
      "\t\t\tSaving storm 46\n",
      "\t\t\tSaving storm 47\n",
      "\t\t\tSaving storm 48\n",
      "\t\t\tSaving storm 49\n",
      "\t\t\tSaving storm 50\n",
      "\t\t\tSaving storm 51\n",
      "\t\t\tSaving storm 52\n",
      "\t\t\tSaving storm 53\n",
      "\t\t\tSaving storm 54\n",
      "\t\t\tSaving storm 55\n",
      "\t\t\tSaving storm 56\n",
      "\t\t\tSaving storm 57\n",
      "\t\t\tSaving storm 58\n",
      "\t\t\tSaving storm 59\n",
      "\t\t\tSaving storm 60\n",
      "\t\t\tSaving storm 61\n",
      "\t\t\tSaving storm 62\n",
      "\t\t\tSaving storm 63\n",
      "\t\t\tSaving storm 64\n",
      "\t\t\tSaving storm 65\n",
      "\t\tValidation storms ./data/sym_h_storms/validation_storms\n",
      "\t\t\tSaving storm 66\n",
      "\t\t\tSaving storm 67\n",
      "\t\t\tSaving storm 68\n",
      "\t\t\tSaving storm 69\n",
      "\t\t\tSaving storm 70\n",
      "\t\t\tSaving storm 71\n",
      "\t\t\tSaving storm 72\n",
      "\t\t\tSaving storm 73\n",
      "\t\t\tSaving storm 74\n",
      "\t\t\tSaving storm 75\n",
      "\t\t\tSaving storm 76\n",
      "\t\t\tSaving storm 77\n",
      "\t\t\tSaving storm 78\n",
      "\t\t\tSaving storm 79\n",
      "\t\t\tSaving storm 80\n",
      "\t\tTest storms ./data/sym_h_storms/test_storms\n",
      "\t\t\tSaving storm 81\n",
      "\t\t\tSaving storm 82\n",
      "\t\t\tSaving storm 83\n",
      "\t\t\tSaving storm 84\n",
      "\t\t\tSaving storm 85\n",
      "\t\t\tSaving storm 86\n",
      "\t\t\tSaving storm 87\n",
      "\t\t\tSaving storm 88\n",
      "\t\t\tSaving storm 89\n",
      "\t\t\tSaving storm 90\n",
      "\t\t\tSaving storm 91\n",
      "\t\t\tSaving storm 92\n",
      "\t\t\tSaving storm 93\n",
      "\t\t\tSaving storm 94\n",
      "\t\t\tSaving storm 95\n",
      "\t\t\tSaving storm 96\n",
      "\t\t\tSaving storm 97\n",
      "\t\t\tSaving storm 98\n",
      "\t\t\tSaving storm 99\n",
      "\t\t\tSaving storm 100\n",
      "\t\t\tSaving storm 101\n",
      "\t\t\tSaving storm 102\n",
      "\t\t\tSaving storm 103\n",
      "\t\t\tSaving storm 104\n",
      "\t\t\tSaving storm 105\n",
      "\t\t\tSaving storm 106\n",
      "\t\t\tSaving storm 107\n",
      "\t\t\tSaving storm 108\n",
      "\t\t\tSaving storm 109\n",
      "\t\t\tSaving storm 110\n",
      "\t\t\tSaving storm 111\n",
      "\t\t\tSaving storm 112\n",
      "\t\t\tSaving storm 113\n",
      "\t\t\tSaving storm 114\n",
      "\t\t\tSaving storm 115\n",
      "\t\t\tSaving storm 116\n",
      "\t\tTest key storms ./data/sym_h_storms/test_key_storms\n",
      "\t\t\tSaving storm 117\n",
      "\t\t\tSaving storm 118\n",
      "\t\t\tSaving storm 119\n",
      "\t\t\tSaving storm 120\n",
      "\t\t\tSaving storm 121\n",
      "\t\t\tSaving storm 122\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Saving raw data to\", download_base_path)\n",
    "\n",
    "start_date = pd.to_datetime(\"1998-01-01\", utc=True)\n",
    "end_date = pd.to_datetime(\"2023-01-01\", utc=True)\n",
    "\n",
    "print(\n",
    "    f\"Downloading SYM-H data from {start_date} until {end_date}\"\n",
    ")\n",
    "print(\n",
    "    f\"Data will be downloaded from the 'OMNI_HRO_5MIN' dataset with a 5 minute resolution\"\n",
    ")\n",
    "\n",
    "status, data = cdas.get_data(\n",
    "    \"OMNI_HRO_5MIN\",\n",
    "    [\"SYM_H\"],\n",
    "    start_date,\n",
    "    end_date,\n",
    "    dataRepresentation=dr.XARRAY,\n",
    ")\n",
    "\n",
    "sym_index = pd.DataFrame(data=data[\"Epoch\"], columns=[\"datetime\"])\n",
    "sym_index[\"datetime\"] = pd.to_datetime(sym_index[\"datetime\"])\n",
    "sym_index[\"SYM_H\"] = data[\"SYM_H\"].values\n",
    "sym_index = sym_index.set_index(\"datetime\")\n",
    "# Remove the UTC formatting to ease the use\n",
    "sym_index.index = sym_index.index.tz_localize(None)\n",
    "sym_index[\"SYM_H\"] = sym_index[\"SYM_H\"].replace(99999, np.nan)\n",
    "\n",
    "sym_index.loc[storm_dates.INVALID_DATES_SYM, \"SYM_H\"] = np.nan\n",
    "sym_index[[\"SYM_H\"]] = sym_index[[\"SYM_H\"]].interpolate()\n",
    "\n",
    "print(f\"Saving data to {os.path.join(download_base_path, 'sym_index.pkl')}\")\n",
    "\n",
    "# Save the DataFrame to a pickle file with gzip compression\n",
    "sym_index.to_pickle(os.path.join(download_base_path, 'sym_index.pkl'), compression=\"gzip\")\n",
    "\n",
    "del sym_index\n",
    "\n",
    "print(\"\\tDownloading SYM storms\")\n",
    "\n",
    "sym_path = os.path.join(download_base_path, \"sym_h_storms\")\n",
    "\n",
    "\n",
    "os.makedirs(train_data_path, exist_ok=True)\n",
    "os.makedirs(val_data_path, exist_ok=True)\n",
    "os.makedirs(test_data_path, exist_ok=True)\n",
    "os.makedirs(test_key_data_path, exist_ok=True)\n",
    "\n",
    "td = datetime.timedelta(days=2)\n",
    "\n",
    "storm_index_all = 1\n",
    "\n",
    "print(f\"\\t\\tTrain storms {train_data_path}\")\n",
    "for st_index in range(len(storm_dates.TRAIN_STORMS)):\n",
    "    start_date = utils.convert_to_datetime(storm_dates.TRAIN_STORMS[st_index][0], utc=True) - td    \n",
    "    end_date = utils.convert_to_datetime(storm_dates.TRAIN_STORMS[st_index][1], utc=True) + td\n",
    "\n",
    "    try:\n",
    "        df_ace = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_ACE_IMF_16,\n",
    "            utils.VARS_ACE_IMF_H0,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_ace.to_csv(\n",
    "            os.path.join(train_data_path, f\"ace_imf_storm{storm_index_all}.csv\")\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting ACE IMF data for training storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    try:\n",
    "        df_swepam = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_ACE_SWEPAM,\n",
    "            utils.VARS_ACE_SWEPAM,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_swepam.to_csv(\n",
    "            os.path.join(\n",
    "                train_data_path, f\"ace_swepam_storm{storm_index_all}.csv\"\n",
    "            )\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting ACE SWEPAM data for training storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    try:\n",
    "        df_omni = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_OMNI,\n",
    "            utils.VARS_OMNI,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_omni.to_csv(\n",
    "            os.path.join(train_data_path, f\"omni_storm{storm_index_all}.csv\")\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting OMNI data for training storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    print(\"\\t\\t\\tSaving storm\", storm_index_all)\n",
    "    storm_index_all += 1\n",
    "\n",
    "print(f\"\\t\\tValidation storms {val_data_path}\")\n",
    "for st_index in range(len(storm_dates.VALIDATION_STORMS)):\n",
    "    start_date = utils.convert_to_datetime(storm_dates.VALIDATION_STORMS[st_index][0], utc=True) - td\n",
    "    end_date = utils.convert_to_datetime(storm_dates.VALIDATION_STORMS[st_index][1], utc=True) + td\n",
    "\n",
    "    try:\n",
    "        df_ace = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_ACE_IMF_16,\n",
    "            utils.VARS_ACE_IMF_H0,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_ace.to_csv(\n",
    "            os.path.join(val_data_path, f\"ace_imf_storm{storm_index_all}.csv\")\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting ACE IMF data for valing storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    try:\n",
    "        df_swepam = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_ACE_SWEPAM,\n",
    "            utils.VARS_ACE_SWEPAM,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_swepam.to_csv(\n",
    "            os.path.join(val_data_path, f\"ace_swepam_storm{storm_index_all}.csv\")\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting ACE SWEPAM data for valing storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)        \n",
    "\n",
    "    try:\n",
    "        df_omni = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_OMNI,\n",
    "            utils.VARS_OMNI,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_omni.to_csv(\n",
    "            os.path.join(val_data_path, f\"omni_storm{storm_index_all}.csv\")\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting OMNI data for valing storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    print(\"\\t\\t\\tSaving storm\", storm_index_all)\n",
    "    storm_index_all += 1\n",
    "\n",
    "print(f\"\\t\\tTest storms {test_data_path}\")\n",
    "for st_index in range(len(storm_dates.TEST_STORMS)):\n",
    "    start_date = utils.convert_to_datetime(storm_dates.TEST_STORMS[st_index][0], utc=True) - td    \n",
    "    end_date = utils.convert_to_datetime(storm_dates.TEST_STORMS[st_index][1], utc=True) + td    \n",
    "\n",
    "    try:\n",
    "        df_ace = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_ACE_IMF_16,\n",
    "            utils.VARS_ACE_IMF_H0,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_ace.to_csv(\n",
    "            os.path.join(test_data_path, f\"ace_imf_storm{storm_index_all}.csv\")\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting ACE IMF data for testing storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    try:\n",
    "        df_swepam = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_ACE_SWEPAM,\n",
    "            utils.VARS_ACE_SWEPAM,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_swepam.to_csv(\n",
    "            os.path.join(test_data_path, f\"ace_swepam_storm{storm_index_all}.csv\")\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting ACE SWEPAM data for testing storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    try:\n",
    "        df_omni = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_OMNI,\n",
    "            utils.VARS_OMNI,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_omni.to_csv(\n",
    "            os.path.join(test_data_path, f\"omni_storm{storm_index_all}.csv\")\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting OMNI data for testing storm {storm_index_all}\\n\"\n",
    "            f\"\\tFrom: {start_date} \\tto: {end_date}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    print(\"\\t\\t\\tSaving storm\", storm_index_all)\n",
    "    storm_index_all += 1\n",
    "\n",
    "print(f\"\\t\\tTest key storms {test_key_data_path}\")\n",
    "for st_index in range(len(storm_dates.TEST_KEY_STORMS)):\n",
    "    start_date = utils.convert_to_datetime(storm_dates.TEST_KEY_STORMS[st_index][0], utc=True) - td\n",
    "    end_date = utils.convert_to_datetime(storm_dates.TEST_KEY_STORMS[st_index][1], utc=True) + td\n",
    "    try:\n",
    "        df_ace = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_ACE_IMF_PROVISIONAL,\n",
    "            utils.VARS_ACE_IMF_PROVISIONAL,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_ace.to_csv(\n",
    "            os.path.join(\n",
    "                test_key_data_path,\n",
    "                \"ace_imf_storm\" + str(storm_index_all) + \".csv\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting provisional ACE imf data for test key storm {storm_index_all}\\n\"\n",
    "            + f\"\\tFrom: {utils.convert_to_datetime(storm_dates.TEST_KEY_STORMS[st_index][0])} \"\n",
    "            + f\"\\tto: {utils.convert_to_datetime(storm_dates.TEST_KEY_STORMS[st_index][1])}\"\n",
    "        )\n",
    "        print(ex)\n",
    "    try:\n",
    "        df_swepam = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_ACE_PLASMA_PROVISIONAL,\n",
    "            utils.VARS_ACE_SWEPAM_PROVISIONAL,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_swepam.to_csv(\n",
    "            os.path.join(\n",
    "                test_key_data_path,\n",
    "                \"ace_swepam_storm\" + str(storm_index_all) + \".csv\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting provisional ACE swepam data for test key storm {storm_index_all}\\n\"\n",
    "            + f\"\\tFrom: {utils.convert_to_datetime(storm_dates.TEST_KEY_STORMS[st_index][0])} \"\n",
    "            + f\"\\tto: {utils.convert_to_datetime(storm_dates.TEST_KEY_STORMS[st_index][1])}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    try:\n",
    "        df_omni = utils.download_data(\n",
    "            cdas,\n",
    "            utils.DATABASE_OMNI,\n",
    "            utils.VARS_OMNI,\n",
    "            start_date,\n",
    "            end_date,\n",
    "        )\n",
    "        df_omni.to_csv(\n",
    "            os.path.join(\n",
    "                test_key_data_path, \"omni_storm\" + str(storm_index_all) + \".csv\"\n",
    "            )\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        print(\n",
    "            f\"Error getting OMNI data for test key storm {storm_index_all}\\n\"\n",
    "            + f\"\\tFrom: {utils.convert_to_datetime(storm_dates.TEST_KEY_STORMS[st_index][0])} \"\n",
    "            + f\"\\tto: {utils.convert_to_datetime(storm_dates.TEST_KEY_STORMS[st_index][1])}\"\n",
    "        )\n",
    "        print(ex)\n",
    "\n",
    "    print(\"\\t\\t\\tSaving storm\", storm_index_all)\n",
    "    storm_index_all = storm_index_all + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565a8e1",
   "metadata": {},
   "source": [
    "# Preprocess and save data data <a class=\"anchor\" id=\"section_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e59cad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T10:43:33.723313Z",
     "start_time": "2024-01-17T10:43:29.629181Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dfs_ace_imf = utils.read_data(train_data_path, pattern='imf')\n",
    "val_dfs_ace_imf = utils.read_data(val_data_path, pattern='imf')\n",
    "test_dfs_ace_imf = utils.read_data(test_data_path, pattern='imf')\n",
    "test_key_dfs_ace_imf = utils.read_data(test_key_data_path, pattern='imf')\n",
    "\n",
    "for i,storm in enumerate(train_dfs_ace_imf):\n",
    "    train_dfs_ace_imf[i] = utils.preprocess_ace_imf(storm)\n",
    "\n",
    "for i,storm in enumerate(val_dfs_ace_imf):\n",
    "    val_dfs_ace_imf[i] = utils.preprocess_ace_imf(storm)\n",
    "\n",
    "for i,storm in enumerate(test_dfs_ace_imf):\n",
    "    test_dfs_ace_imf[i] = utils.preprocess_ace_imf(storm)\n",
    "    \n",
    "for i,storm in enumerate(test_key_dfs_ace_imf):\n",
    "    test_key_dfs_ace_imf[i] = utils.preprocess_ace_imf_provisional(storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4104534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T10:43:35.022014Z",
     "start_time": "2024-01-17T10:43:33.724384Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dfs_ace_swepam = utils.read_data(train_data_path, pattern='swepam')\n",
    "val_dfs_ace_swepam = utils.read_data(val_data_path, pattern='swepam')\n",
    "test_dfs_ace_swepam = utils.read_data(test_data_path, pattern='swepam')\n",
    "test_key_dfs_ace_swepam = utils.read_data(test_key_data_path, pattern='swepam')\n",
    "\n",
    "for i,storm in enumerate(train_dfs_ace_swepam):\n",
    "    train_dfs_ace_swepam[i] = utils.preprocess_ace_swepam(storm)\n",
    "\n",
    "for i,storm in enumerate(val_dfs_ace_swepam):\n",
    "    val_dfs_ace_swepam[i] = utils.preprocess_ace_swepam(storm)\n",
    "\n",
    "for i,storm in enumerate(test_dfs_ace_swepam):\n",
    "    test_dfs_ace_swepam[i] = utils.preprocess_ace_swepam(storm)\n",
    "    \n",
    "for i,storm in enumerate(test_key_dfs_ace_swepam):\n",
    "    test_key_dfs_ace_swepam[i] = utils.preprocess_ace_swepam_provisional(storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b04e26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T10:43:35.280328Z",
     "start_time": "2024-01-17T10:43:35.023004Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dfs_omni = utils.read_data(train_data_path, pattern='omni')\n",
    "val_dfs_omni = utils.read_data(val_data_path, pattern='omni')\n",
    "test_dfs_omni = utils.read_data(test_data_path, pattern='omni')\n",
    "test_key_dfs_omni = utils.read_data(test_key_data_path, pattern = 'omni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ebc455b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T10:43:35.413105Z",
     "start_time": "2024-01-17T10:43:35.281657Z"
    }
   },
   "outputs": [],
   "source": [
    "train_storms = []\n",
    "val_storms = []\n",
    "test_storms = []\n",
    "test_key_storms = []\n",
    "\n",
    "\n",
    "for i in range(len(train_dfs_omni)):\n",
    "    storm = train_dfs_ace_imf[i].join(train_dfs_ace_swepam[i])\n",
    "    storm = storm.join(train_dfs_omni[i][INDEX_COL])\n",
    "    train_storms.append(storm)\n",
    "    \n",
    "for i in range(len(val_dfs_omni)):\n",
    "    storm = val_dfs_ace_imf[i].join(val_dfs_ace_swepam[i])\n",
    "    storm = storm.join(val_dfs_omni[i][INDEX_COL])\n",
    "    val_storms.append(storm)\n",
    "    \n",
    "for i in range(len(test_dfs_omni)):\n",
    "    storm = test_dfs_ace_imf[i].join(test_dfs_ace_swepam[i])\n",
    "    storm = storm.join(test_dfs_omni[i][INDEX_COL])\n",
    "    test_storms.append(storm)\n",
    "    \n",
    "for i in range(len(test_key_dfs_omni)):\n",
    "    storm = test_key_dfs_ace_imf[i].join(test_key_dfs_ace_swepam[i])\n",
    "    storm['Proton_speed_x'] = -storm['Proton_speed']\n",
    "    storm = storm.join(test_key_dfs_omni[i][INDEX_COL])    \n",
    "    test_key_storms.append(storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7232f91f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T10:43:37.527251Z",
     "start_time": "2024-01-17T10:43:35.414219Z"
    }
   },
   "outputs": [],
   "source": [
    "global_index_storm = 1\n",
    "\n",
    "for i,storm in enumerate(train_storms):\n",
    "    storm.to_csv(os.path.join(train_data_path_preprocessed, f'train_storm_{global_index_storm}.csv'))\n",
    "    global_index_storm += 1\n",
    "    \n",
    "for i,storm in enumerate(val_storms):\n",
    "    storm.to_csv(os.path.join(val_data_path_preprocessed, f'val_storm_{global_index_storm}.csv'))\n",
    "    global_index_storm += 1\n",
    "    \n",
    "for i,storm in enumerate(test_storms):\n",
    "    storm.to_csv(os.path.join(test_data_path_preprocessed, f'test_storm_{global_index_storm}.csv'))\n",
    "    global_index_storm += 1\n",
    "\n",
    "for i,storm in enumerate(test_key_storms):\n",
    "    storm.to_csv(os.path.join(test_key_data_path_preprocessed, f'test_key_storm_{global_index_storm}.csv'))\n",
    "    global_index_storm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e396f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tf2-14",
   "language": "python",
   "name": "tf2-14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
